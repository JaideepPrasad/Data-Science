{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FashionMNIST Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import Lambda, ToTensor\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download training data from open datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset FashionMNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: data\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: ToTensor()"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data = datasets.FashionMNIST(root='data',\n",
    "                                      train=True,\n",
    "                                      download=True,\n",
    "                                      transform=ToTensor()) # download Fashion MNIST training dataset\n",
    "training_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download test data from open datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset FashionMNIST\n",
       "    Number of datapoints: 10000\n",
       "    Root location: data\n",
       "    Split: Test\n",
       "    StandardTransform\n",
       "Transform: ToTensor()"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_data = datasets.FashionMNIST(root='data',\n",
    "                                      train=False,\n",
    "                                      download=True,\n",
    "                                      transform=ToTensor()) # download Fashion MNIST testing dataset\n",
    "testing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dl = DataLoader(training_data,\n",
    "                      batch_size = 64,\n",
    "                      shuffle = True) # load partial data so we don't crash the system\n",
    "\n",
    "test_dl = DataLoader(testing_data,\n",
    "                      batch_size = 64,\n",
    "                      shuffle = True) # load partial data so we don't crash the system\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 1, 28, 28]), torch.Size([64]))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features, train_labels = next(iter(train_dl)) # this is if you want to look at the data  inside as you can't call train_dl directly\n",
    "test_features, test_labels = next(iter(test_dl))\n",
    "\n",
    "train_features.shape, train_labels.shape            # 64 rows, 1 col, 28 x 28 pixels\n",
    "test_features.shape, test_labels.shape              # 64 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get cpu or gpu device for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' # if we have a gpu, we will use that. If not we will use cpu\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn \n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.NN = nn.Sequential(nn.Flatten(),\n",
    "                                nn.Linear(28*28, 450),\n",
    "                                nn.LogSigmoid(),\n",
    "                                nn.Linear(450, 100),\n",
    "                                nn.Hardtanh(),\n",
    "                                nn.Linear(100, 10),\n",
    "                                nn.Hardswish())\n",
    "\n",
    "    def forward(self, x):\n",
    "        #logits = self.NN(x)                       # it's logits if the last layer is not an activation function, else its pred\n",
    "        #pred = nn.Softmax(dim=1)(logits)\n",
    "        pred = self.NN(x)\n",
    "        return pred\n",
    "\n",
    "model = NeuralNetwork().to(device) # create the model using the device (cpu/gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing the Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):        \n",
    "        X = X.to(device)      # use gpu by moving data to gpu\n",
    "        y = y.to(device)\n",
    "        \n",
    "        # Compute prediction and loss\n",
    "        preds = model(X)\n",
    "        loss = loss_fn(preds, y)\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad() # set gradients to zero\n",
    "        loss.backward()       # recalculate gradients with new values for weights and biases\n",
    "        optimizer.step()      # adjust the weights and biases based on new gradients \n",
    "\n",
    "        if batch % 100 == 0:  # Book keeping to see our progress\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():        # disable autograds\n",
    "        for X, y in dataloader:\n",
    "            X = X.to(device)     # use gpu by moving data to gpu\n",
    "            y = y.to(device)\n",
    "            \n",
    "            # Compute prediction, accuracy and loss\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y)\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum()\n",
    "            \n",
    "    test_loss /= size\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.304689  [    0/60000]\n",
      "loss: 2.297739  [ 6400/60000]\n",
      "loss: 2.284675  [12800/60000]\n",
      "loss: 2.297423  [19200/60000]\n",
      "loss: 2.282977  [25600/60000]\n",
      "loss: 2.291730  [32000/60000]\n",
      "loss: 2.280680  [38400/60000]\n",
      "loss: 2.286123  [44800/60000]\n",
      "loss: 2.276484  [51200/60000]\n",
      "loss: 2.263857  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 35.8%, Avg loss: 0.035647 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.272923  [    0/60000]\n",
      "loss: 2.273957  [ 6400/60000]\n",
      "loss: 2.260088  [12800/60000]\n",
      "loss: 2.252306  [19200/60000]\n",
      "loss: 2.247666  [25600/60000]\n",
      "loss: 2.247673  [32000/60000]\n",
      "loss: 2.235411  [38400/60000]\n",
      "loss: 2.225111  [44800/60000]\n",
      "loss: 2.212979  [51200/60000]\n",
      "loss: 2.197710  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 42.3%, Avg loss: 0.034668 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 2.196602  [    0/60000]\n",
      "loss: 2.207886  [ 6400/60000]\n",
      "loss: 2.193743  [12800/60000]\n",
      "loss: 2.173895  [19200/60000]\n",
      "loss: 2.138376  [25600/60000]\n",
      "loss: 2.132745  [32000/60000]\n",
      "loss: 2.127644  [38400/60000]\n",
      "loss: 2.092941  [44800/60000]\n",
      "loss: 2.062456  [51200/60000]\n",
      "loss: 2.053085  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 49.7%, Avg loss: 0.032005 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 2.028380  [    0/60000]\n",
      "loss: 1.970944  [ 6400/60000]\n",
      "loss: 1.963192  [12800/60000]\n",
      "loss: 1.873875  [19200/60000]\n",
      "loss: 1.889162  [25600/60000]\n",
      "loss: 1.856029  [32000/60000]\n",
      "loss: 1.819452  [38400/60000]\n",
      "loss: 1.760518  [44800/60000]\n",
      "loss: 1.696805  [51200/60000]\n",
      "loss: 1.687916  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 54.7%, Avg loss: 0.025808 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.603195  [    0/60000]\n",
      "loss: 1.561118  [ 6400/60000]\n",
      "loss: 1.576676  [12800/60000]\n",
      "loss: 1.541884  [19200/60000]\n",
      "loss: 1.536101  [25600/60000]\n",
      "loss: 1.391705  [32000/60000]\n",
      "loss: 1.492838  [38400/60000]\n",
      "loss: 1.446767  [44800/60000]\n",
      "loss: 1.427418  [51200/60000]\n",
      "loss: 1.437438  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 64.4%, Avg loss: 0.021494 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.390172  [    0/60000]\n",
      "loss: 1.443435  [ 6400/60000]\n",
      "loss: 1.228888  [12800/60000]\n",
      "loss: 1.270537  [19200/60000]\n",
      "loss: 1.369305  [25600/60000]\n",
      "loss: 1.233384  [32000/60000]\n",
      "loss: 1.224888  [38400/60000]\n",
      "loss: 1.319425  [44800/60000]\n",
      "loss: 1.203822  [51200/60000]\n",
      "loss: 1.209703  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 67.6%, Avg loss: 0.019126 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.122976  [    0/60000]\n",
      "loss: 1.108812  [ 6400/60000]\n",
      "loss: 1.174001  [12800/60000]\n",
      "loss: 1.225677  [19200/60000]\n",
      "loss: 1.067538  [25600/60000]\n",
      "loss: 1.182483  [32000/60000]\n",
      "loss: 1.116166  [38400/60000]\n",
      "loss: 1.220167  [44800/60000]\n",
      "loss: 1.097999  [51200/60000]\n",
      "loss: 1.095729  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 69.0%, Avg loss: 0.017415 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 1.155577  [    0/60000]\n",
      "loss: 1.145267  [ 6400/60000]\n",
      "loss: 1.195699  [12800/60000]\n",
      "loss: 1.075573  [19200/60000]\n",
      "loss: 1.073712  [25600/60000]\n",
      "loss: 0.847797  [32000/60000]\n",
      "loss: 1.077230  [38400/60000]\n",
      "loss: 1.012872  [44800/60000]\n",
      "loss: 1.004768  [51200/60000]\n",
      "loss: 1.123732  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 69.5%, Avg loss: 0.016100 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 1.062449  [    0/60000]\n",
      "loss: 0.988936  [ 6400/60000]\n",
      "loss: 1.049550  [12800/60000]\n",
      "loss: 0.928584  [19200/60000]\n",
      "loss: 1.014171  [25600/60000]\n",
      "loss: 0.916587  [32000/60000]\n",
      "loss: 0.823726  [38400/60000]\n",
      "loss: 1.096769  [44800/60000]\n",
      "loss: 0.917208  [51200/60000]\n",
      "loss: 0.699438  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 0.015042 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.946613  [    0/60000]\n",
      "loss: 0.899862  [ 6400/60000]\n",
      "loss: 0.993385  [12800/60000]\n",
      "loss: 0.915510  [19200/60000]\n",
      "loss: 0.751298  [25600/60000]\n",
      "loss: 0.923304  [32000/60000]\n",
      "loss: 1.003772  [38400/60000]\n",
      "loss: 0.969669  [44800/60000]\n",
      "loss: 0.958285  [51200/60000]\n",
      "loss: 0.931931  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 71.7%, Avg loss: 0.014158 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.926335  [    0/60000]\n",
      "loss: 0.916950  [ 6400/60000]\n",
      "loss: 0.842768  [12800/60000]\n",
      "loss: 1.033084  [19200/60000]\n",
      "loss: 0.842269  [25600/60000]\n",
      "loss: 0.862526  [32000/60000]\n",
      "loss: 0.886619  [38400/60000]\n",
      "loss: 1.042000  [44800/60000]\n",
      "loss: 0.825760  [51200/60000]\n",
      "loss: 0.747559  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 72.3%, Avg loss: 0.013463 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.784407  [    0/60000]\n",
      "loss: 0.840645  [ 6400/60000]\n",
      "loss: 0.821190  [12800/60000]\n",
      "loss: 0.722179  [19200/60000]\n",
      "loss: 0.947131  [25600/60000]\n",
      "loss: 0.760390  [32000/60000]\n",
      "loss: 0.872842  [38400/60000]\n",
      "loss: 0.827444  [44800/60000]\n",
      "loss: 0.872090  [51200/60000]\n",
      "loss: 0.861688  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 72.8%, Avg loss: 0.012866 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.731438  [    0/60000]\n",
      "loss: 0.779489  [ 6400/60000]\n",
      "loss: 0.793694  [12800/60000]\n",
      "loss: 0.853668  [19200/60000]\n",
      "loss: 0.818681  [25600/60000]\n",
      "loss: 0.643751  [32000/60000]\n",
      "loss: 0.747410  [38400/60000]\n",
      "loss: 0.660630  [44800/60000]\n",
      "loss: 0.776245  [51200/60000]\n",
      "loss: 0.894166  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 73.5%, Avg loss: 0.012369 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.641268  [    0/60000]\n",
      "loss: 0.729822  [ 6400/60000]\n",
      "loss: 0.743840  [12800/60000]\n",
      "loss: 0.792277  [19200/60000]\n",
      "loss: 0.649087  [25600/60000]\n",
      "loss: 0.776190  [32000/60000]\n",
      "loss: 0.808594  [38400/60000]\n",
      "loss: 0.691099  [44800/60000]\n",
      "loss: 0.747229  [51200/60000]\n",
      "loss: 0.762089  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 73.8%, Avg loss: 0.012026 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.785897  [    0/60000]\n",
      "loss: 0.892573  [ 6400/60000]\n",
      "loss: 0.787719  [12800/60000]\n",
      "loss: 0.773141  [19200/60000]\n",
      "loss: 0.599508  [25600/60000]\n",
      "loss: 0.696088  [32000/60000]\n",
      "loss: 0.766410  [38400/60000]\n",
      "loss: 0.655155  [44800/60000]\n",
      "loss: 0.714617  [51200/60000]\n",
      "loss: 0.793086  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 74.0%, Avg loss: 0.011681 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.614227  [    0/60000]\n",
      "loss: 0.779299  [ 6400/60000]\n",
      "loss: 0.753004  [12800/60000]\n",
      "loss: 0.737984  [19200/60000]\n",
      "loss: 0.721848  [25600/60000]\n",
      "loss: 0.652315  [32000/60000]\n",
      "loss: 0.703310  [38400/60000]\n",
      "loss: 0.567346  [44800/60000]\n",
      "loss: 0.746041  [51200/60000]\n",
      "loss: 0.630154  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 74.6%, Avg loss: 0.011370 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.610610  [    0/60000]\n",
      "loss: 0.715782  [ 6400/60000]\n",
      "loss: 0.724759  [12800/60000]\n",
      "loss: 0.691824  [19200/60000]\n",
      "loss: 0.653064  [25600/60000]\n",
      "loss: 0.691502  [32000/60000]\n",
      "loss: 0.750824  [38400/60000]\n",
      "loss: 0.873919  [44800/60000]\n",
      "loss: 0.799313  [51200/60000]\n",
      "loss: 0.582808  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 74.7%, Avg loss: 0.011151 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.708358  [    0/60000]\n",
      "loss: 0.594600  [ 6400/60000]\n",
      "loss: 0.758520  [12800/60000]\n",
      "loss: 0.776876  [19200/60000]\n",
      "loss: 0.729277  [25600/60000]\n",
      "loss: 0.842522  [32000/60000]\n",
      "loss: 0.740235  [38400/60000]\n",
      "loss: 0.794408  [44800/60000]\n",
      "loss: 0.721256  [51200/60000]\n",
      "loss: 0.641313  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 75.0%, Avg loss: 0.010957 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.677798  [    0/60000]\n",
      "loss: 0.531375  [ 6400/60000]\n",
      "loss: 0.735294  [12800/60000]\n",
      "loss: 0.821076  [19200/60000]\n",
      "loss: 0.495466  [25600/60000]\n",
      "loss: 0.822754  [32000/60000]\n",
      "loss: 0.690941  [38400/60000]\n",
      "loss: 0.620081  [44800/60000]\n",
      "loss: 0.622645  [51200/60000]\n",
      "loss: 0.912645  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 75.2%, Avg loss: 0.010762 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.844355  [    0/60000]\n",
      "loss: 0.793055  [ 6400/60000]\n",
      "loss: 0.631160  [12800/60000]\n",
      "loss: 0.612226  [19200/60000]\n",
      "loss: 0.642775  [25600/60000]\n",
      "loss: 0.710017  [32000/60000]\n",
      "loss: 0.531565  [38400/60000]\n",
      "loss: 0.583373  [44800/60000]\n",
      "loss: 0.563953  [51200/60000]\n",
      "loss: 0.668188  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 75.7%, Avg loss: 0.010608 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.671161  [    0/60000]\n",
      "loss: 0.628118  [ 6400/60000]\n",
      "loss: 0.632322  [12800/60000]\n",
      "loss: 0.650255  [19200/60000]\n",
      "loss: 0.739594  [25600/60000]\n",
      "loss: 0.787763  [32000/60000]\n",
      "loss: 0.782680  [38400/60000]\n",
      "loss: 0.558810  [44800/60000]\n",
      "loss: 0.625350  [51200/60000]\n",
      "loss: 0.685318  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 75.7%, Avg loss: 0.010462 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.652076  [    0/60000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.669377  [ 6400/60000]\n",
      "loss: 0.621105  [12800/60000]\n",
      "loss: 0.749178  [19200/60000]\n",
      "loss: 0.608201  [25600/60000]\n",
      "loss: 0.571381  [32000/60000]\n",
      "loss: 0.523625  [38400/60000]\n",
      "loss: 0.919148  [44800/60000]\n",
      "loss: 0.555771  [51200/60000]\n",
      "loss: 0.557145  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 76.1%, Avg loss: 0.010336 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.612837  [    0/60000]\n",
      "loss: 0.688603  [ 6400/60000]\n",
      "loss: 0.621635  [12800/60000]\n",
      "loss: 0.629625  [19200/60000]\n",
      "loss: 0.601384  [25600/60000]\n",
      "loss: 0.567466  [32000/60000]\n",
      "loss: 0.864176  [38400/60000]\n",
      "loss: 0.520633  [44800/60000]\n",
      "loss: 0.677145  [51200/60000]\n",
      "loss: 0.612717  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 76.5%, Avg loss: 0.010200 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.735146  [    0/60000]\n",
      "loss: 0.614797  [ 6400/60000]\n",
      "loss: 0.637342  [12800/60000]\n",
      "loss: 0.492802  [19200/60000]\n",
      "loss: 0.734672  [25600/60000]\n",
      "loss: 0.718053  [32000/60000]\n",
      "loss: 0.734683  [38400/60000]\n",
      "loss: 0.586277  [44800/60000]\n",
      "loss: 0.567852  [51200/60000]\n",
      "loss: 0.594477  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 76.4%, Avg loss: 0.010117 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.561041  [    0/60000]\n",
      "loss: 0.489012  [ 6400/60000]\n",
      "loss: 0.551406  [12800/60000]\n",
      "loss: 0.632567  [19200/60000]\n",
      "loss: 0.678004  [25600/60000]\n",
      "loss: 0.697159  [32000/60000]\n",
      "loss: 0.615847  [38400/60000]\n",
      "loss: 0.654304  [44800/60000]\n",
      "loss: 0.781616  [51200/60000]\n",
      "loss: 0.585528  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 76.7%, Avg loss: 0.009995 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 25         \n",
    "learning_rate = 1e-3 # 0.001\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()                                    # create loss function\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  # create optimizer\n",
    "\n",
    "for t in range(epochs):                                            \n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    # train_loop\n",
    "    train_loop(train_dl, model, loss_fn, optimizer)\n",
    "    \n",
    "    #test_loop\n",
    "    test_loop(test_dl, model, loss_fn)\n",
    "    \n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "\n",
    "torch.save(model.state_dict(), 'model_parameters.pth') # save model parameters into a file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (NN): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=784, out_features=450, bias=True)\n",
       "    (2): LogSigmoid()\n",
       "    (3): Linear(in_features=450, out_features=100, bias=True)\n",
       "    (4): Hardtanh(min_val=-1.0, max_val=1.0)\n",
       "    (5): Linear(in_features=100, out_features=10, bias=True)\n",
       "    (6): Hardswish()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model = model                                              # call our model that we created before\n",
    "loaded_model.load_state_dict(torch.load('model_parameters.pth'))  # load the parameters from file to model\n",
    "loaded_model.eval()                                               # sets self.train to false for certian hidden layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model can now be used to make predictions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: \"Trouser\", Actual: \"Trouser\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-65-6d40e98a0f8d>:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x, y = torch.tensor(x).to(device), torch.tensor(y).to(device)  # move data to gpu if used\n"
     ]
    }
   ],
   "source": [
    "classes = [\n",
    "    \"T-shirt/top\",\n",
    "    \"Trouser\",\n",
    "    \"Pullover\",\n",
    "    \"Dress\",\n",
    "    \"Coat\",\n",
    "    \"Sandal\",\n",
    "    \"Shirt\",\n",
    "    \"Sneaker\",\n",
    "    \"Bag\",\n",
    "    \"Ankle boot\",\n",
    "]                                                              # we define which class represent which object\n",
    "\n",
    "model.eval()                                                   # sets self.train to false for certian hidden layers\n",
    "x, y = test_features[0], test_labels[0]                        # choose the first random data point in data set\n",
    "x, y = torch.tensor(x).to(device), torch.tensor(y).to(device)  # move data to gpu if used\n",
    "with torch.no_grad():                                          # disable autograd\n",
    "    pred = model(x)\n",
    "    predicted, actual = classes[pred[0].argmax(0)], classes[y] # retrive predicted and actual clothing name\n",
    "    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
